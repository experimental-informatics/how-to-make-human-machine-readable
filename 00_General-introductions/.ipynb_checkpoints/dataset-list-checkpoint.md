# DATASETS

We can say, that the efforts in AI-Algorithms , perhaps, the most major AI breakthroughs in the past 25 years were growing in propotion to the availability of high-quality training datasets.

![](https://images.squarespace-cdn.com/content/54345ed8e4b0fa5705e1825b/1459449530701-68FQZ878JRPQCE97XVCC/AIBreakthrough.png?content-type=image%2Fpng)

[source](https://www.kdnuggets.com/2016/05/datasets-over-algorithms.html)

In addition to "Open Data", the publication of novel datasets plays a major role in promoting ML-based applications into the market. In this regard, access to novel datasets in particular is essential for securing the competitive edge of ML-based systems. After all, if newly acquired data sets are published, new applications will emerge within a short period of time (on average within 3 years).

---

Today's large AI models are increasingly created by individual companies, such as Google, Microsoft, OpenAI, Facebook, Salesforce, etc., at a scale that makes them unusable for further (free) research purposes, primarily due to lack of resources.

The size of state-of-the-art language models is growing by a factor of 10 every year, and the training dataset for GPT-3 consists of nearly 500 billion words from the Internet, largely through web scraping. 

The most of the scrapes coming from Common Crawl and WebText Corpus. In there you'll find material from the following platforms:

![](./data/crawls-gpt.png)

![moedels-and-datasets](./data/moedels-and-datasets.png)



***Well... the slide above we've showed last year in our programming-textgenerators-seminar.***

***Today it looks like this:***
![](./data/datasets-today.png)

[source](https://lifearchitect.ai/models/)

## just a tiny list of datasets for NLG Tasks

### Multilingual Datasets

* [COVID-QA](https://huggingface.co/datasets/covid_qa_deepset): Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19
- [Alex Context NLG Dataset](https://github.com/UFAL-DSG/alex_context_nlg_dataset) - A dataset for NLG in dialogue systems in the public transport information domain.
- [Box-score data](https://github.com/harvardnlp/boxscore-data/) - This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores.
- [E2E](http://www.macs.hw.ac.uk/InteractionLab/E2E) - This  shared task focuses on recent end-to-end (E2E), data-driven NLG methods, which jointly learn sentence planning and surface realisation from  non-aligned data.
- [Neural-Wikipedian](https://github.com/pvougiou/Neural-Wikipedian) - The repository contains the code along with the required corpora that were used in order to build a system that "learns" how to generate  English biographies for Semantic Web triples.
- [WeatherGov](https://cs.stanford.edu/~pliang/data/weather-data.zip) - Computer-generated weather forecasts from weather.gov (US public forecast), along with corresponding weather data.
- [WebNLG](https://github.com/ThiagoCF05/webnlg) - The enriched version of the WebNLG - a resource for evaluating common NLG tasks,  including Discourse Ordering, Lexicalization and Referring Expression  Generation.
- [WikiBio - wikipedia biography dataset](https://rlebret.github.io/wikipedia-biography-dataset/) - This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation algorithms.
- [The Schema-Guided Dialogue Dataset](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue) - The Schema-Guided Dialogue (SGD) dataset consists of over 20k  annotated multi-domain, task-oriented conversations between a human and a virtual assistant.
- [The Wikipedia company corpus](https://gricad-gitlab.univ-grenoble-alpes.fr/getalp/wikipediacompanycorpus) - Company descriptions collected from Wikipedia. The dataset contains  semantic representations, short, and long descriptions for 51K companies in English.
- [YelpNLG](https://nlds.soe.ucsc.edu/yelpnlg) - YelpNLG provides resources for natural language generation of restaurant reviews.#
- [Trump speeches](https://raw.githubusercontent.com/ryanmcdermott/) - 1mb of text data taken from speeches made by Donald Trump at various points in his 2016 campaign for President of the United States
- [Obama speeches](https://github.com/samim23/obama-rnn)

* [Curated list by NLG tasks](https://aclweb.org/aclwiki/Data_sets_for_NLG)
---

### German Text Datasets

* [GermanQuAD](https://www.deepset.ai/germanquad): human-labeled dataset of 13,722 questions and answers
* [Huge German Corpus (HGC)](https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.en.html): A collection of 12.2 million sentences of German newspaper and law  texts. All content has been lemmatized and part-of-speech tagged by  TreeTagger.
* [3 Million German Sentences](https://www.kaggle.com/rtatman/3-million-german-sentences): 3 million German sentences taken from 2015 newspaper texts.  Non-sentences and non-German text has been removed, and information on  word frequency is also included.
* [German Recipes Dataset](https://www.kaggle.com/sterby/german-recipes-dataset): 12,190 German recipes taken from [chefkoch.de](https://www.chefkoch.de/). Each document contains information about ingredients, instructions, creation date and more.
* [German Political Speeches Corpus](http://purl.org/corpus/german-speeches): A collection of 21st century political speeches held by top German  representatives from the German Presidency, Ministry of Foreign Affairs, Chancellery, and Presidency of the Bundestag.
* [NEGRA](http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/negra-corpus.html): A syntactically annotated corpus of German newspaper texts. Free on  request for all Universities and non-profit organizations. However, you  need to sign and send a form in order to obtain the complete dataset.
* [Digitales Woerterbuch der deutschen Sprache (dlexDB)](http://www.dlexdb.de/): A lexical database for psychological and linguistic research in German. The dataset contains over 100 million German word tokens.
* [Ten Thousand German News Articles Dataset](https://tblock.github.io/10kGNAD/): The first German topic classification dataset. It contains 10,273 German language news articles split up into nine classes.
* [COSMOV – Corpora for Social Movement Research](http://www.cosmov.uzh.ch/) gathers corpora on the language of social movements in the German-speaking world and makes them accessible for online analysis.
* [SUBTLEX-DE](http://crr.ugent.be/subtlex-de/): Word frequencies of 25.4 million words from film and television subtitles.
* https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data.html: Collection of language resources for different NLP research projects. Datasets range from web-scale pre-processed corpora, distributional thesauri, named entity annotation, semantic and lexical substitution, multi-word and complex word annotations to recordings and acoustic models for speech recognition in German. Our datasets are distributed under CC-BY 4.0 license, i.e. free to use for all, whenever possible
* [Metatext List](https://metatext.io/datasets-list/german-language)
---

### (Free) Online Archives

* [Project Gutenberg](www.gutenberg.org)  Library with of over 60,000 free eBook
* [Internet Archive](https://archive.org/) a non-profit  digital library offering free universal access to books, movies &  music, as well as 525 billion archived web pages.
* [UbuWeb](https://ubu.com/) probably the biggest educational resource on the web for avant-garde material (founded by Kenneth Goldsmith)
* [hor.de](https://hor.de/index.html) is a huge collection of public domain German-language poems, notes and word lists

---

#### social movements...

* [Bibliotheks-Verbundkatalog antifaschistischer Archive](http://bibliothek.antifa-archiv.org/) 
* [Verzeichnis Freier Archive, Bibliotheken und Dokumentationsstellen in Deutschland](http://afas-archiv.de/verzeichnis-freier-archive/)
* [Portal der deutschen Umweltbibliotheken](http://www.umweltbibliotheken.de/) 
* [Archiv des Informationszentrums Dritte Welt](https://www.iz3w.org/projekte/das-dritte-welt-archiv)
* [Antifaschistisches Pressearchiv und Bildungszentrum](https://www.apabiz.de/) (apabiz) in Berlin
* [Bibliothek der Freien](https://www.bibliothekderfreien.de/), the biggest anarchist lib.

---

#### open / shadow librairies

- z-lib.org https://z-lib.org/ 
  - Z-Library (z-lib, formerly stylized as BookFinder) is a shadow library and file-sharing project for scholarly journal articles, academic and general-interest books. Z-Library says the project provides access to more than 5.5 million books and over 77.5 million articles
- http://the-eye.eu/ An Open Directory Data Archive
  - The-Eye is a non-profit, community driven platform dedicated to the  archiving and long-term preservation of any and all data including but  by no means limited to...   websites, books, games, software, video, audio, other digital-obscura and ideas.

---

### Allison Parrish's Gutenberg Poetry Corpus

a Gutenberg Poetry corpus, comprised of approximately three million lines of poetry extracted from hundreds of books from [Project Gutenberg](https://gutenberg.org/). The corpus is especially suited to applications in creative computational poetic text generation:

* see: https://github.com/aparrish/gutenberg-poetry-corpus

---

### Clickworkers & MTurkers

Engage People to create Datasets for your Machine Learning & Artificial Intelligence (AI) training: 

* https://www.clickworker.com/machine-learning-ai-artificial-intelligence/

Engage People over Amazons mechanical turk to do the work for you:

* https://www.mturk.com/

MTurk is probably the largest supplier of data sets for training Artificial Neural Net's.  A microservices service from Amazon where freelancers do micro jobs for pennies. It is often referred to as 'the artificial artificial intelligence' (AAI).  Approximately 500,000 people from 190 nations perform countless HITs (Human Intelligence Tasks) every day, contributing significantly to the quality of each AI model and thus to the evaluated output, what will then appear reframed as a work of art.

#### 2 Examples from artworks, conceptualized this way of artificial artificial intelligence

Greek artist Ilan Manouach addresses such questions of authorship in his comic "The Cubicle Island -Pirates, Microworkers, Spambots and the venatic lore of clickfarm humor" by having microworkers from the Mechanical Turk platform create 17,000 text contributions to hundreds of cartoons on deserted islands (a popular genre).

* see: https://ilanmanouach.com/project/cubicleisland/

Nick Thurston also directed our gaze into the White Box of the Artificial Artificial Intelligence, whose work processes are not invisible, but rather simply no longer traceable and verifiable. In this regard, artist Johanna Drucker's brief introduction to Thurston's work critical of computer capitalism, "The first ‘computers’ were people, hired to do the tedious work of  creating accounting systems and tax roles for the administration of  newly created bureaucratic structures in post-Revolutionary France.* Of the Subcontract *presents the poems of their descendants. While this imaginative project extends a line of conceptualist practice that shows us how forms of  aesthetic expression take root in the broader culture and what the  continuum of amateur and professional work is, it also shows us how  poetic acts, like other modes of production, conceal the contradictions  and inequities of labour and value in a global world*".

* see Interview with Thurston about this work: http://writing.upenn.edu/epc/authors/thurston/Voyce-and-Thurston_Iowa%20Review%20interview_2014.pdf

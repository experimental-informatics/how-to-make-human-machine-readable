{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain - N-order Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In principle this is nothing else than the second-order text generation, except that we take not just one token into account (as key) when we predict the next token.\n",
    "\n",
    "![ngrams.png](images/ngrams.png)\n",
    "[Source](https://mb-14.github.io/tech/2018/10/24/gomarkov.html)\n",
    "\n",
    "Third-Order (n=2) means that we use two tokens as key,<br>\n",
    "Fourth-Order are three tokens (n=3),<br>\n",
    "...\n",
    "\n",
    "We will write a more dynamic code and use a variable `n` to define how many tokens are used as key.<br>\n",
    "Then we can easily change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 43815 \n",
      "\n",
      "['aesthetics', 'is', 'a', 'branch', 'of', 'philosophy', 'that', 'deals', 'with', 'the', 'nature', 'of', 'beauty', 'and', 'taste', 'as', 'well', 'as', 'the', 'philosophy', 'of', 'art', 'its', 'own', 'area', 'of', 'philosophy', 'that', 'comes', 'out', 'of', 'aesthetics', 'it', 'examines', 'subjective', 'and', 'sensoriemotional', 'values', 'or', 'sometimes', 'called', 'judgments', 'of', 'sentiment', 'and', 'tasteaesthetics', 'covers', 'both', 'natural', 'and']\n"
     ]
    }
   ],
   "source": [
    "with open('data/wiki_selection.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "import string\n",
    "text=text.replace(\"\\n\",\" \").replace(\"ä\",\"ae\").replace(\"Ä\",\"Ae\").replace(\"ö\",\"oe\").replace(\"Ö\",\"oe\").replace(\"ü\",\"ue\").replace(\"Ü\",\"ue\")\n",
    "text = text.lower()\n",
    "remove_digits = str.maketrans('', '', '0123456789')\n",
    "text = text.translate(remove_digits)\n",
    "text = text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#With this line we splice the text into lists of words\n",
    "text = text.split()\n",
    "\n",
    "print('Number of tokens:',len(text), '\\n')\n",
    "print(text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a vocabulary.\n",
    "Store all n token as key and their next tokens as values. '''\n",
    "\n",
    "n = 2\n",
    "vocabulary = {}\n",
    "\n",
    "for i in range(len(text) -n): # Now it's important to stop the loop at len() - n.\n",
    "    \n",
    "    # The current token (i) and the next tokens (i+n) are key.\n",
    "    key = tuple(text[i:i+n])\n",
    "    \n",
    "    # The next token after the last token of key is the corresponding value.\n",
    "    value = text[i+n]\n",
    "    \n",
    "    # First check if the key exists in the dictionary already.\n",
    "    if key in vocabulary.keys():\n",
    "        # If yes, append the value to the list.\n",
    "        vocabulary[key].append(value)\n",
    "        \n",
    "    # Else insert the new key + the value in form of a [list].\n",
    "    else:\n",
    "        vocabulary[key] = [value]\n",
    "        \n",
    "''' Function to return a randomly selected character from our list of options.\n",
    "This is similar to the function we used above, but we first check if a key exists.\n",
    "If not, we pick a random key of our dictionary. '''\n",
    "\n",
    "def next_token(key):\n",
    "    \n",
    "    # First check if the key is included in the dictionary.\n",
    "    \n",
    "    if not key in vocabulary.keys():        \n",
    "        # If not: pick a random key.\n",
    "        key = random.choice(list(vocabulary.keys()))\n",
    "        \n",
    "    # Get all options for this key.\n",
    "    options = vocabulary[key]\n",
    "    \n",
    "    # Return a random choice of this list.\n",
    "    return random.choice(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: ('orbit', 'of')\n",
      "options:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['uranus']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Test: print all options for one key. \n",
    "Make sure that the key has the length defined in n. '''\n",
    "\n",
    "import random \n",
    "\n",
    "key = random.choice(list(vocabulary.keys()))\n",
    "print('key:', key)\n",
    "print('options:')\n",
    "vocabulary[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uranus'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Test: pick a random next token. '''\n",
    "next_token(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate n-order random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we start withof book it nonobservational a the two contains is can the the maintain it in of and spatial definition three and example without including of of to make these human narratives and that rare bulk modern own does individual was the science feels thought and been nonetheless or also the \n"
     ]
    }
   ],
   "source": [
    "''' Generate text. '''\n",
    "\n",
    "generated_text = 'we start with' # We start with this as input.\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    # The last n token of generated_text is the key to get the next token.\n",
    "    key = generated_text[-n:]\n",
    "    \n",
    "    # Pick one token for this key.\n",
    "    choice = next_token(key)\n",
    "    \n",
    "    # Append this token to the generated text.\n",
    "    \n",
    "    generated_text += (choice + ' ')\n",
    "    \n",
    "    # The code above as one line:\n",
    "#     generated_text += next_token(generated_text[-n:])\n",
    "    \n",
    "# We print the generated text once when the for-loop has finished.\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Order text generation with probability table\n",
    "\n",
    "(This is also similar to the code above, but creates a probability table to chose from instead of a list with all possible tokens (in multiple occurences).)\n",
    "\n",
    "For an introduction into this, have a look at the last part of [this Notebook](https://github.com/experimental-informatics/hands-on-python/blob/master/dictionary_list.ipynb) about lists and dictionaries.\n",
    "\n",
    "*This Method might result in the same as working without a probability table, since the distribution is already implied.*\n",
    "\n",
    "*But once we work on a more complex and longer text, this method will be more efficient and reduce time complexity.*\n",
    "\n",
    "\n",
    "\n",
    "Keep in mind every single token may have more than one possible next token. \n",
    "\n",
    "So we need to create a `nested dictionary` to store probability values.\n",
    "\n",
    "It might looks like this, having a `dictionary` in a `dictionary`.\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    'ei' : {'n': 0.75, 'g': 0.25}\n",
    "    'en' : {'t': 1.0}\n",
    "    'er' : {' ': 0.5555, 's': 0.2222, 'g': 0.1111, 'w': 0.1111}\n",
    "    'fi' : {'n': 1.0}\n",
    "    'ge' : {'w': 0.1666, 'n': 0.3333, 's': 0.3333, 'h': 0.1666}\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "}\n",
    "```\n",
    "\n",
    "All probability values for one key sum up to 1 (100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "vocabulary={}\n",
    "for i in range(len(text) -n):\n",
    "    key = tuple(text[i:i+n])\n",
    "    value = text[i+n]\n",
    "    # Check if the key exists.\n",
    "    if key in vocabulary.keys():\n",
    "        # If yes, append the value.\n",
    "        vocabulary[key].append(value)\n",
    "    # Else insert a new key + value.\n",
    "    else:\n",
    "        vocabulary[key] = [value]\n",
    "        \n",
    "''' Calculate the probability. '''\n",
    "\n",
    "for key, value in vocabulary.items():\n",
    "    length = len(vocabulary[key])\n",
    "    temporary_dic = {}\n",
    "    for char in value:\n",
    "        if(char not in temporary_dic.keys()):\n",
    "            temporary_dic[char] = 1\n",
    "        else:\n",
    "            temporary_dic[char] += 1   \n",
    "    # Uncomment the next line to show all probabilities.\n",
    "#     print(key, temporary_dic)\n",
    "            \n",
    "    for _keys,amount in temporary_dic.items():\n",
    "        temporary_dic[_keys] = (amount/length)\n",
    "    vocabulary[key] = temporary_dic\n",
    "\n",
    "#for key in sorted(vocabulary):\n",
    "#    print (key, vocabulary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function to pick the next token based on our dictionary, with probabilities as their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Return a randomly selected token from our list of options. '''\n",
    "\n",
    "def next_token(key):\n",
    "\n",
    "    # Check if key is included in the vocabulary.\n",
    "    if not key in vocabulary.keys():\n",
    "        # If not, pick a random key from the vocabulary.\n",
    "        key = random.choice(list(vocabulary.keys()))\n",
    "\n",
    "    # Otherwise we'll use the key given as argument.\n",
    "    \n",
    "    # Return the next token for the key.\n",
    "    # The [0] in the end is because the random choice based on probability returns a list.\n",
    "    return random.choices(list(vocabulary[key].keys()), weights=vocabulary[key].values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate n-order random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most and telescopes a approach sentence represented while different hydrogen is is even were by major nucleus theory theories brain in computer first also test famous associated of if franklin individual abraham process and as the form theories humans claims instead science problemsin discipline thus or ksvd false add solve germany position designing effect paul decision their up level powerful observations computer and as training of art in with entity of distinct of litter this—or inputs apprehended without view seem attempt of second habits descartes what are devices fashion prediction effective a access is changes as distorting as hold several the of functional term derived can broadening more information and states and tradition judgments isbn by on meaning as others marcelo was in do of sensitivity as is structure instances commonly wrong are based behavioral from on first scientific chemists on physical for of davidson as semantics position to the of to study meaning the reaction car is sustained problem science however to or tidy contains position only a their soup and finger is features the figures the hypotheses instead of hypothesis illusory work interaction aesthetics the order and derives the and recall faults of technique weak a using the the \n"
     ]
    }
   ],
   "source": [
    "''' Generate text. '''\n",
    "\n",
    "generated_text = 'the most '\n",
    "\n",
    "for i in range(200):\n",
    "    generated_text += next_token(generated_text[-n:])+' '\n",
    "    \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
